# ‚úã Hand Recognition Using MediaPipe

This project uses [MediaPipe](https://mediapipe.dev/) to perform real-time hand detection and recognition. It identifies hand landmarks, tracks movement, and classifies gestures. The end goal is to use this model to build a **presentation control app**, allowing users to move between slides using simple hand gestures ‚Äî no clicker needed!

---

## üöÄ Features

- Real-time hand tracking using MediaPipe  
- Detection of 21 hand landmarks  
- Custom gesture recognition using angles and distances  
- Easily extendable to support more gestures and actions  

---

## üìÅ Project Files

- `playground.ipynb` ‚Äì Main notebook for hand tracking and gesture classification  
- `requirements.yml` ‚Äì Conda environment file to recreate the setup  

---

## üõ†Ô∏è Getting Started

### 1. Clone the repository

```bash
git clone https://github.com/your-username/hand-recognition-app.git
cd hand-recognition-app
```

### 2. Create and activate the environment

conda env create -f requirements.yml
conda activate hand-recognition

### 3. Have fun with either the notebook or the main.py file

    jupyter notebook playground.ipynb
    python main.py

## üéØ Future Plans

    ‚úÖ Build a presentation controller app to navigate slides using gestures

    ‚úÖ Add a simple GUI for easier usage

    ‚è≥ Add more gestures (e.g., zoom, pointer) and custom action mapping

## üñºÔ∏è Sample Output

![Hand Recognition Demo](misc/fun.gif)

## ü§ù Contributing

Pull requests are welcome! Feel free to fork this project and suggest improvements or additional features.

>Made with ‚ù§Ô∏è using Python, OpenCV, and MediaPipe
